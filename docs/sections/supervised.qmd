# Supervised learning

* Data splitting (if a training/test set split is enough for the global analysis, at least one CV or bootstrap must be used)
* Two or more models 
* Two or more scores
* Tuning of one or more hyperparameters per model
* Interpretation of the model(s)

## Model 1
This section provides a comprehensive outline of the linear regression model and analysis methods employed in the study of property price determinants.

### Tools and Software
The study was conducted using R, a programming language and environment widely recognized for its robust capabilities in statistical computing and graphics. Key packages used include:

- `corrplot` for visualization of correlation matrices, aiding in preliminary feature selection.
car for diagnostic testing and variance inflation factor (VIF) analysis to detect multicollinearity among predictors.
- `caret` for creating training and testing sets, and managing cross-validation processes.
- `ggplot2` and `plotly` for creating visual representations of model diagnostics, predictions, and residuals.
- `gtsummary` for creating publication-ready tables summarizing regression analysis results.

Each of these tools was chosen for its specific functionality that aids in robust data analysis, ensuring that each step of the model building and evaluation process is well-supported.

### Linear Regression Overall
#### Correlation Analysis

Initially, a correlation analysis was conducted to identify and visualize linear relationships between the property prices and potential predictive variables such as the number of rooms and square meters. The correlation matrix was computed and plotted using the `corrplot` package:

```{r}
correlation_matrix <- cor(properties_filtered[ , c("price", "number_of_rooms", "square_meters")], use="complete.obs")
corrplot(correlation_matrix, method="square", type="upper", tl.col="black", tl.srt=45, tl.cex=0.8, cl.cex=0.8, addCoef.col="black", number.cex=0.8, order="hclust", hclust.method="complete", tl.pos="lt", tl.offset=0.5, cl.pos="n", cl.length=1.5)
```

This step helped in highlighting significant predictors for the regression model.

#### Multicollinearity Check
To ensure the stability of the regression model, a multicollinearity check was performed using the Variance Inflation Factor (VIF), calculated through the car package. High VIF values would indicate predictors that have strong linear relationships with other predictors, thus potentially destabilizing the model:
```{r}
## Multicollinearity Check
model_for_vif <- lm(price ~ number_of_rooms + square_meters + canton + floor + year_category , data=properties_filtered)
vif_values <- vif(model_for_vif)
#show the result in the html
kable(vif_values, format = "html") %>%
  kable_styling(full_width = F)
```

#### Model Building

The dataset was split into training and testing sets to validate the modelâ€™s performance. The linear regression model was then fitted using selected predictors:
```{r}
# Model Building
## Split the Data
set.seed(123)  # for reproducibility
trainIndex <- createDataPartition(properties_filtered$price, p=0.8, list=FALSE)
trainData <- properties_filtered[trainIndex, ]
testData <- properties_filtered[-trainIndex, ]

## Fit the Model
lm_model <- lm(price ~ number_of_rooms + square_meters + property_type + floor + year_category , data=trainData)

```

#### Model Evaluation
Diagnostic checks such as residual analysis and normality tests were conducted to validate model assumptions. Performance metrics including R-squared and RMSE were calculated to assess the model's explanatory power and prediction accuracy.
```{r}
# Model Evaluation
## Diagnostic Checks
plot(lm_model)
ad.test(residuals(lm_model))

#use gt summary to show the result
tbl_reg_1 <- gtsummary::tbl_regression(lm_model)
tbl_reg_1
```

#### Model testing

Stepwise regression was performed to refine the model and improve its predictive performance. The resulting model was evaluated using the same diagnostic checks and performance metrics as the initial model:

```{r}
# stepwise regression
lm_model2 <- step(lm_model)
plot(lm_model2)
ad.test(residuals(lm_model2))

## Performance Metrics
print(summary(lm_model2)$r.squared)
print(summary(lm_model2)$adj.r.squared)
print(rmse(testData$price, predict(lm_model2, newdata=testData)))
print(mae(testData$price, predict(lm_model2, newdata=testData)))

#use gt summary to show the result 
tbl_reg_2 <- gtsummary::tbl_regression(lm_model2)
tbl_reg_3 <- tbl_merge(
  tbls= list(tbl_reg_1, tbl_reg_2),
  tab_spanner = c("**Model 1**", "**Mode Stepwise**")
  )
tbl_reg_3
```

#### Cross-Validation

Cross-validation was performed to enhance the robustness of the model, using the caret package to manage this process efficiently:

```{r}
## Cross-Validation
cv_results <- train(price ~ number_of_rooms + square_meters + year_category + property_type, data=trainData, method="lm", trControl=trainControl(method="cv", number=10))
summary(cv_results)
#show the CV result in the html
kable(cv_results$results, format = "html") %>%
  kable_styling(full_width = F)
```

#### Model testing

The final model was tested using the unseen test dataset to evaluate its predictive accuracy. Residual plots and actual vs. predicted price plots were created to visually assess model performance:

##### Residual vs Predicted Prices
```{r}
# Model Testing 
## Test the Model
predicted_prices <- predict(lm_model2, newdata=testData)
testData$predicted_prices <- predicted_prices  # Add to testData to ensure alignment
# Calculate residuals
testData$test_residuals <- testData$price - predicted_prices  # Manually compute residuals

# Residual Analysis
gg <- ggplot(data = testData, aes(x = predicted_prices, y = test_residuals)) +
  geom_point() +
  geom_smooth(method = "lm", color = "blue") +
  labs(title = "Residuals vs Predicted Prices", x = "Predicted Prices", y = "Residuals")

# Convert ggplot to plotly
p <- ggplotly(gg, width = 600, height = 400)

# Show the interactive plot
p
```

##### Actual vs Predicted Prices
```{r}
## Visualization
gg <- ggplot(data=testData, aes(x=predicted_prices, y=price)) +
    geom_point() +
    geom_smooth(method="lm", col="blue") +
    labs(title="Actual vs Predicted Prices", x="Predicted Prices", y="Actual Prices")

# Convert ggplot to plotly
p <- ggplotly(gg, width = 600, height = 400)

# Show the interactive plot
p
```

### Linear Regression between 10th and 90th percentile
#### Correlation Analysis
```{r}
#filter properties_filtered based on the 10th percentile and 90th percentile of the data
properties_filtered <- properties_filtered %>% 
  filter(price >= quantile(price, 0.1) & price <= quantile(price, 0.9))
```

```{r}
correlation_matrix <- cor(properties_filtered[ , c("price", "number_of_rooms", "square_meters")], use="complete.obs")
corrplot(correlation_matrix, method="square", type="upper", tl.col="black", tl.srt=45, tl.cex=0.8, cl.cex=0.8, addCoef.col="black", number.cex=0.8, order="hclust", hclust.method="complete", tl.pos="lt", tl.offset=0.5, cl.pos="n", cl.length=1.5)
```

```{r}
## Multicollinearity Check
model_for_vif <- lm(price ~ number_of_rooms + square_meters + canton + floor + year_category , data=properties_filtered)
vif_values <- vif(model_for_vif)
#show the result in the html
kable(vif_values, format = "html") %>%
  kable_styling(full_width = F)
```

#### Model Building
```{r}
# Model Building
## Split the Data
set.seed(123)  # for reproducibility
trainIndex <- createDataPartition(properties_filtered$price, p=0.8, list=FALSE)
trainData <- properties_filtered[trainIndex, ]
testData <- properties_filtered[-trainIndex, ]

## Fit the Model
lm_model <- lm(price ~ number_of_rooms + square_meters + property_type + floor + year_category , data=trainData)

```

#### Model Evaluation
```{r}
# Model Evaluation
## Diagnostic Checks
plot(lm_model)
ad.test(residuals(lm_model))

#use gt summary to show the result
tbl_reg_1 <- gtsummary::tbl_regression(lm_model)
tbl_reg_1
```

#### Model testing
```{r}
# stepwise regression
lm_model2 <- step(lm_model)
plot(lm_model2)
ad.test(residuals(lm_model2))

## Performance Metrics
print(summary(lm_model2)$r.squared)
print(summary(lm_model2)$adj.r.squared)
print(rmse(testData$price, predict(lm_model2, newdata=testData)))
print(mae(testData$price, predict(lm_model2, newdata=testData)))

#use gt summary to show the result 
tbl_reg_2 <- gtsummary::tbl_regression(lm_model2)
tbl_reg_3 <- tbl_merge(
  tbls= list(tbl_reg_1, tbl_reg_2),
  tab_spanner = c("**Model 1**", "**Mode Stepwise**")
  )
tbl_reg_3
```

#### Cross-Validation
```{r}
## Cross-Validation
cv_results <- train(price ~ number_of_rooms + square_meters + year_category + property_type, data=trainData, method="lm", trControl=trainControl(method="cv", number=10))
summary(cv_results)
#show the CV result in the html
kable(cv_results$results, format = "html") %>%
  kable_styling(full_width = F)
```

#### Model testing
##### Residual vs Predicted Prices
```{r}
# Model Testing 
## Test the Model
predicted_prices <- predict(lm_model2, newdata=testData)
testData$predicted_prices <- predicted_prices  # Add to testData to ensure alignment
# Calculate residuals
testData$test_residuals <- testData$price - predicted_prices  # Manually compute residuals

# Residual Analysis
gg <- ggplot(data = testData, aes(x = predicted_prices, y = test_residuals)) +
  geom_point() +
  geom_smooth(method = "lm", color = "blue") +
  labs(title = "Residuals vs Predicted Prices", x = "Predicted Prices", y = "Residuals")

# Convert ggplot to plotly
p <- ggplotly(gg, width = 600, height = 400)

# Show the interactive plot
p
```

##### Actual vs Predicted Prices
```{r}
## Visualization
gg <- ggplot(data=testData, aes(x=predicted_prices, y=price)) +
    geom_point() +
    geom_smooth(method="lm", col="blue") +
    labs(title="Actual vs Predicted Prices", x="Predicted Prices", y="Actual Prices")

# Convert ggplot to plotly
p <- ggplotly(gg, width = 600, height = 400)

# Show the interactive plot
p
```