# Supervised learning

* Data splitting (if a training/test set split is enough for the global analysis, at least one CV or bootstrap must be used)
* Two or more models 
* Two or more scores
* Tuning of one or more hyperparameters per model
* Interpretation of the model(s)

## Model 1

This section provides a comprehensive outline of the linear regression model and analysis methods employed in the study of property price determinants.

### Tools 

For the linear regression model, R programming language and its packages were used `corrplot` was used for visualizing correlation matrices, `car` for checking multicollinearity through variance inflation factor (VIF), `caret` for managing data splits and cross-validation, `ggplot2` and `plotly` for graphical representations of diagnostics and results, and `gtsummary` for summarizing the regression outcomes in tables. These tools supported each phase of the model, from preliminary analysis to validation.

### Linear Regression

We analyzed property price determinants using linear regression, involving data splitting, correlation analysis for predictor selection, and VIF for multicollinearity checks. The model was refined with stepwise regression, and its performance evaluated using RMSE, MAE, and R-squared through cross-validation to ensure robustness and generalizability.

### Linear Regression - With All Prices
#### Correlation Analysis - Continuous Variable

Initially, a correlation analysis was conducted to identify and visualize linear relationships between the property prices and potential predictive variables such as the number of rooms and square meters. The correlation matrix was computed and plotted using the `corrplot` package:

```{r}
# Create a copy of properties_filtered as properties2
properties2 <- properties_filtered

# Convert property_type column to a factor in properties2
properties2$property_type <- as.factor(properties2$property_type)
# Convert the factor to numeric scale
properties2$property_type_numeric <- as.numeric(properties2$property_type)

# Convert canton column to a factor in properties2
properties2$canton <- as.factor(properties2$canton)
# Convert the factor to numeric scale
properties2$canton_numeric <- as.numeric(properties2$canton)

# Convert canton column to a factor in properties2
properties2$year_category <- as.factor(properties2$year_category)
# Convert the factor to numeric scale
properties2$year_category_numeric <- as.numeric(properties2$year_category)

# Convert canton column to a factor in properties2
properties2$Demographic_cluster <- as.factor(properties2$Demographic_cluster)
# Convert the factor to numeric scale
properties2$Demographic_cluster_numeric <- as.numeric(properties2$Demographic_cluster)

# Convert canton column to a factor in properties2
properties2$Tax_cluster <- as.factor(properties2$Tax_cluster)
# Convert the factor to numeric scale
properties2$Tax_cluster_numeric <- as.numeric(properties2$Tax_cluster)

# Convert canton column to a factor in properties2
properties2$Political_cluster <- as.factor(properties2$Political_cluster)
# Convert the factor to numeric scale
properties2$Political_cluster_numeric <- as.numeric(properties2$Political_cluster)

correlation_matrix <- cor(properties2[ , c("price", "number_of_rooms", "square_meters","property_type_numeric","canton_numeric", "year_category_numeric", "Political_cluster_numeric","Tax_cluster_numeric","Demographic_cluster_numeric")], use="complete.obs")
corrplot(correlation_matrix, method="square", type="upper", tl.col="black", tl.srt=45, tl.cex=0.8, cl.cex=0.8, addCoef.col="black", number.cex=0.8, order="hclust", hclust.method="complete", tl.pos="lt", tl.offset=0.5, cl.pos="n", cl.length=1.5)
```

- We can observe that the correlation between the number of rooms and price (0.46) and square meters and price (0.67) suggests a moderate correlation with the number of rooms and a strong correlation with square meters. 
- The number of rooms and square meters also have a strong correlation (0.7), indicating potential multicollinearity between these predictors.
- There is a correlation of 0.28 between the type of properties and price, suggesting a weak positive relationship. 
- The correlation of 0.08 between canton and price indicates a very weak positive relationship. 
- Surprisingly, there are almost no correlation (-0.03) between the year category and price.
- Further analysis reveals that the year category exhibits correlations of -0.31, -0.28, and -0.17 with the type of properties, number of rooms, and square meters, respectively. These negative correlations suggest that as the year category increases, there is a tendency for the type of properties, number of rooms, and square meters to decrease, which may reflect changing trends or market dynamics over time.

#### GVIF (Generalized Variance Inflation Factor)
```{r}
properties_filtered
## Multicollinearity Check
model_for_vif <- lm(price ~ number_of_rooms + square_meters + canton + floor + year_category + Demographic_cluster + Political_cluster + Tax_cluster, data=properties_filtered)
vif_values <- vif(model_for_vif)
#show the result in the html
kable(vif_values, format = "html") %>%
  kable_styling(full_width = F)
```

- No multicollinearity issues are observed for the predictors number_of_rooms and square_meters, with VIF values below 5 as though they are correlated, the correlation is not strong enough to cause multicollinearity issues.
- High VIF Values:

    - canton and Tax_cluster have VIF values much greater than 10, indicating serious multicollinearity issues. These predictors are highly correlated with other predictors in the model.

- Moderate VIF Values:
  
    - Other predictors like number_of_rooms, square_meters, Political_cluster, etc., have VIF values below 5, indicating acceptable multicollinearity.

We removed the `canton` variable from the model due to its high VIF value, which could lead to unstable coefficient estimates and unreliable model predictions.

We keep the `Tax_cluster` variable in the model for now, as it may provide valuable information for predicting property prices.

####Basic Model
##### Model Building and Evaluation

The data set was split into training and testing sets to validate the model’s performance. The linear regression model was then fitted using selected predictors:
```{r}
# Model Building
## Split the Data
set.seed(123)  # for reproducibility
trainIndex <- createDataPartition(properties_filtered$price, p=0.8, list=FALSE)
trainData <- properties_filtered[trainIndex, ]
testData <- properties_filtered[-trainIndex, ]

## Fit the Model
lm_model_1 <- lm(price ~ number_of_rooms + square_meters + property_type + floor + year_category  + Demographic_cluster +Political_cluster + Tax_cluster , data=trainData)

summary(lm_model_1)
```

Diagnostic checks such as residual analysis and normality tests were conducted to validate model assumptions. Performance metrics including R-squared and RMSE were calculated to assess the model's explanatory power and prediction accuracy.

```{r}
#use gt summary to show the result
tbl_reg_1 <- gtsummary::tbl_regression(lm_model_1)
tbl_reg_1
```

- Significant Predictors of Price:

    - Square meters: The most influential variable with a strong positive effect on price.
    - Property types: Some types significantly affect prices, with villas and attic flats increasing prices, while single houses and farm houses decrease prices.
    - Year category: Newer properties consistently have higher prices, with significant positive impacts for all categories compared to the baseline.

##### Assess Overfitting

```{r}
# For the Linear Model
lm_train_pred <- predict(lm_model_1, newdata = trainData)
lm_test_pred <- predict(lm_model_1, newdata = testData)

# Calculate RMSE and R-squared for Training Data
lm_train_rmse <- sqrt(mean((trainData$price - lm_train_pred)^2))
lm_train_rsquared <- summary(lm(lm_train_pred ~ trainData$price))$r.squared

# Calculate RMSE and R-squared for Test Data
lm_test_rmse <- sqrt(mean((testData$price - lm_test_pred)^2))
lm_test_rsquared <- summary(lm(lm_test_pred ~ testData$price))$r.squared

# show the results in a table
results_table <- data.frame(
  Model = c("Linear Regression"),
  RMSE_Train = lm_train_rmse,
  RMSE_Test = lm_test_rmse,
  Rsquared_Train = lm_train_rsquared,
  Rsquared_Test = lm_test_rsquared
)

#show table in html
kable(results_table, format = "html") %>%
  kable_styling(position = "center", bootstrap_options = c("striped", "bordered", "hover", "condensed"))
```

No overfitting is observed as the RMSE and R-squared values are similar between the training and test sets, indicating that the model generalizes well to new data.

##### Metrics 

Here are the performance metrics for the initial model:
```{r}
# print R-squared and Adj R-squared and RMSE and MAE and AIC
r_sq <- summary(lm_model_1)$r.squared
adj_r_sq <- summary(lm_model_1)$adj.r.squared
rmse <- rmse(testData$price, predict(lm_model_1, newdata=testData))
mae <- mae(testData$price, predict(lm_model_1, newdata=testData))
aic <- AIC(lm_model_1)
# show those metrics in a html table
metrics_1 <- kable(data.frame(r_sq, adj_r_sq, rmse, mae, aic), format = "html", col.names = c("R-Squared", "Adjusted R-Squared", "RMSE", "MAE", "AIC")) %>%
  kable_styling(position = "center", bootstrap_options = c("striped", "bordered", "hover", "condensed"))  %>%
  add_header_above(c("Basic Model Performance Metrics" = 5)) 
metrics_1
```

- The model has moderate explanatory power, with an R-Squared of 0.493 and an Adjusted R-Squared of 0.492.

- Prediction accuracy, as indicated by RMSE (980,302) and MAE (491,715), shows that the model's predictions are reasonably close to actual prices but could be improved.

- The AIC value (500,701) provides a benchmark for comparing with other models

#### Hyperparameter Tuning
##### Stepwise Regression
Stepwise regression was performed to refine the model and improve its predictive performance. The resulting model was evaluated using the same diagnostic checks and performance metrics as the initial model:

```{r}
# stepwise regression
lm_model_2 <- step(lm_model_1)

#use gt summary to show the result 
tbl_reg_2 <- gtsummary::tbl_regression(lm_model_2)
tbl_reg_1_and_2 <- tbl_merge(
  tbls= list(tbl_reg_1, tbl_reg_2),
  tab_spanner = c("**Basic Model**", "**Model Stepwise**")
  )
tbl_reg_1_and_2
```

We observe :

- Number of Rooms and Floor Dropped

    -  Interestingly the stepwise model drops the number_of_rooms and floor variables, indicating that these predictors may not significantly influence property prices.
- Consistency in Key Predictors 
    
    - The primary significant predictors (e.g., square_meters, certain property types, and year categories) remain consistent across both models, indicating their robust influence on property prices.
- Similar Effect Sizes

    - The effect sizes (β) and confidence intervals for significant predictors are similar in both models, reinforcing the reliability of these predictors.

##### Lasso and Ridge Regression
A Lasso and Ridge regression were also performed to compare the performance of the linear regression model with regularization techniques. 
We fit both models using cross-validation to determine the optimal lambda (penalty parameter).
The plots show the lambda selection process for both Lasso and Ridge models.

```{r, warning=FALSE}

# Convert data frames to matrices for glmnet
dat_tr_re_mat_x <- as.matrix(trainData[, c("number_of_rooms", "square_meters", "floor", "year_category", "Demographic_cluster", "Political_cluster", "Tax_cluster")])
dat_tr_re_mat_y <- trainData$price

dat_te_re_mat_x <- as.matrix(testData[, c("number_of_rooms", "square_meters", "floor", "year_category", "Demographic_cluster", "Political_cluster", "Tax_cluster")])
dat_te_re_mat_y <- testData$price

#fit lasso and ridge
set.seed(123)  # For reproducibility

# Fit Lasso model
lasso_model <- cv.glmnet(dat_tr_re_mat_x, dat_tr_re_mat_y, alpha = 1) # Lasso

# Fit Ridge model
ridge_model <- cv.glmnet(dat_tr_re_mat_x, dat_tr_re_mat_y, alpha = 0) # Ridge

ridge_fit_best <- glmnet(x=dat_tr_re_mat_x, y = dat_tr_re_mat_y, 
                         lambda = ridge_model$lambda.min)

lasso_fit_best <- glmnet(x=dat_tr_re_mat_x, y=dat_tr_re_mat_y, 
                         lambda = lasso_model$lambda.min) #can also use lasso_fit$lambda.1se

# lasso & ridge performance on the training set
postResample(predict(ridge_fit_best, newx = dat_tr_re_mat_x), dat_tr_re_mat_y)
postResample(predict(lasso_fit_best, newx = dat_tr_re_mat_x), dat_tr_re_mat_y)
# lasso & ridge performance on the test set
postResample(predict(ridge_fit_best, newx = dat_te_re_mat_x), dat_te_re_mat_y)
postResample(predict(lasso_fit_best, newx = dat_te_re_mat_x), dat_te_re_mat_y)

# Step-wise lm performance on training and test sets
postResample(predict(lm_model_2,trainData), dat_tr_re_mat_y)
postResample(predict(lm_model_2,testData), dat_te_re_mat_y)
```
We then compared the performance of the Lasso and Ridge models with the stepwise linear regression model using RMSE and MAE:

```{r}
# Calculate RMSE and MAE
get_metrics <- function(predictions, actuals) {
  RMSE <- sqrt(mean((predictions - actuals)^2))
  MAE <- mean(abs(predictions - actuals))
  Rsquared <- cor(predictions, actuals)^2

  
  return(c(RMSE = RMSE, MAE = MAE, Rsquared = Rsquared) )
}

# Capture the performance metrics
ridge_train_preds <- predict(ridge_fit_best, newx = dat_tr_re_mat_x)
lasso_train_preds <- predict(lasso_fit_best, newx = dat_tr_re_mat_x)
ridge_test_preds <- predict(ridge_fit_best, newx = dat_te_re_mat_x)
lasso_test_preds <- predict(lasso_fit_best, newx = dat_te_re_mat_x)
lm_train_preds <- predict(lm_model_2, trainData)
lm_test_preds <- predict(lm_model_2, testData)

ridge_train_metrics <- get_metrics(ridge_train_preds, dat_tr_re_mat_y)
lasso_train_metrics <- get_metrics(lasso_train_preds, dat_tr_re_mat_y)
ridge_test_metrics <- get_metrics(ridge_test_preds, dat_te_re_mat_y)
lasso_test_metrics <- get_metrics(lasso_test_preds, dat_te_re_mat_y)
lm_train_metrics <- get_metrics(lm_train_preds, dat_tr_re_mat_y)
lm_test_metrics <- get_metrics(lm_test_preds, dat_te_re_mat_y)

# Create a data frame with the performance metrics
performance_df <- data.frame(
  Model = c("Ridge (Training)", "Lasso (Training)", "Ridge (Test)", "Lasso (Test)", "Stepwise (Training)", "Stepwise (Test)"),
  RMSE = c(ridge_train_metrics["RMSE"], lasso_train_metrics["RMSE"], ridge_test_metrics["RMSE"], lasso_test_metrics["RMSE"], lm_train_metrics["RMSE"], lm_test_metrics["RMSE"]),
  MAE = c(ridge_train_metrics["MAE"], lasso_train_metrics["MAE"], ridge_test_metrics["MAE"], lasso_test_metrics["MAE"], lm_train_metrics["MAE"], lm_test_metrics["MAE"]),
  Rsquared = c(ridge_train_metrics["Rsquared"], lasso_train_metrics["Rsquared"], ridge_test_metrics["Rsquared"], lasso_test_metrics["Rsquared"], lm_train_metrics["Rsquared"], lm_test_metrics["Rsquared"])
)

# Create the kable extra table
performance_table_hyp_tune <- kable(performance_df, format = "html") %>%
  kable_styling(full_width = FALSE, position = "center", bootstrap_options = c("striped", "bordered", "hover", "condensed")) %>%
  add_header_above(c( "Performance Metrics (RMSE, MAE, R-sq)" = 4))

# Print the table
performance_table_hyp_tune
```

The Stepwise model is the preferred choice for predicting property prices based on the provided metrics. It offers the best balance of accuracy and predictive power. Lasso regression is a good alternative, particularly if model simplicity and interpretability are priorities due to its ability to shrink coefficients and eliminate irrelevant features. Ridge regression, while slightly less accurate, provides stable performance and handles multicollinearity well, good for our tax_cluster variable with high VIF.

##### Metrics

Here we compare the performance metrics of the initial model and the stepwise model.
The metrics of our initial model :
```{r}
metrics_1
```

Stepwise model:
```{r}
# print R-squared and Adj R-squared and RMSE and MAE and AIC
r_sq <- summary(lm_model_2)$r.squared
adj_r_sq <- summary(lm_model_2)$adj.r.squared
rmse <- rmse(testData$price, predict(lm_model_2, newdata=testData))
mae <- mae(testData$price, predict(lm_model_2, newdata=testData))
aic <- AIC(lm_model_2)
# show those metrics in a html table
metrics_2 <- kable(data.frame(r_sq, adj_r_sq, rmse, mae, aic), format = "html", col.names = c("R-Squared", "Adjusted R-Squared", "RMSE", "MAE", "AIC")) %>%
  kable_styling(position = "center", bootstrap_options = c("striped", "bordered", "hover", "condensed"))  %>%
  add_header_above(c("Stepwise Model Performance Metrics" = 5)) 
metrics_2
```

The Stepwise model offers a slight improvement over the Basic model in terms of prediction accuracy (lower MAE) and model efficiency (lower AIC).
Both models perform similarly in explaining the variance in property prices, with nearly identical R-Squared and Adjusted R-Squared values.
Given the minimal differences, the choice between models may depend on the preference for model simplicity (Stepwise model) versus a more comprehensive approach (Basic model).

#### Cross-Validation

Cross-validation was used to assess the model's robustness, typically to avoid overfitting and ensure that the model generalizes well to new data., using the `caret` package to manage this process efficiently. The CV results show metrics like RMSE and R-squared across folds, which indicate the model’s performance stability across different subsets of the data.

10-fold cross-validation Metrics:
```{r}
#add + Demographic_cluster +Political_cluster + Tax_cluster after dealing with NAN
## Cross-Validation
cv_results <- train(price ~ number_of_rooms + square_meters + year_category + property_type + Demographic_cluster +Political_cluster + Tax_cluster , data=trainData, method="lm", trControl=trainControl(method="cv", number=10))
summary(cv_results)


#show the CV result in the html
metrics_cv_1 <- kable(cv_results$results, format = "html") %>%
  kable_styling(position = "center", bootstrap_options = c("striped", "bordered", "hover", "condensed"))  %>%
  add_header_above(c("10 Fold Cross Validation Metrics" = 7))
metrics_cv_1
```

Here are the performance metrics for the stepwise model:
```{r}
metrics_2
```

The 10-fold cross-validation results indicate improved prediction accuracy with a lower RMSE of 955,116 compared to the Stepwise model's RMSE of 980,670. However, the explanatory power is slightly reduced, as shown by a marginally lower R-squared of 0.496 in the cross-validated assessments versus 0.493 in the Stepwise model. These findings suggest that while the model exhibits a stable generalization to new data, it does so with a *slight* compromise in explaining the variance across different data subsets. But the difference is minimal.

#### Model testing

We chose the stepwise model as the best model for the linear regresion due to its balance of accuracy and simplicity.

The final model was tested using the unseen test dataset to evaluate its predictive accuracy. Residual plots and actual vs. predicted price plots were created to visually assess model performance:

##### Residual vs Predicted Prices
This plot  shows residuals (differences between observed and predicted prices) against predicted prices. Ideally, residuals should randomly scatter around the horizontal line at zero, indicating that the model doesn’t systematically overestimate or underestimate prices. 

```{r}
# Model Testing 
## Test the Model
predicted_prices <- predict(lm_model_2, newdata=testData)
testData$predicted_prices <- predicted_prices  # Add to testData to ensure alignment
# Calculate residuals
testData$test_residuals <- testData$price - predicted_prices  # Manually compute residuals

# Residual Analysis
gg <- ggplot(data = testData, aes(x = predicted_prices, y = test_residuals)) +
  geom_point() +
  geom_smooth(method = "lm", color = "blue") +
  labs(title = "Residuals vs Predicted Prices", x = "Predicted Prices", y = "Residuals")

# Convert ggplot to plotly
p <- ggplotly(gg, width = 600, height = 400)

# Show the interactive plot
p
```

The residual plot for all cantons shows a pronounced trend of increasing residuals with higher predicted prices, along with more extreme outliers. This indicates potential model misfit or heteroscedasticity issues when considering all cantons. In contrast, 

##### Actual vs Predicted Prices

This plot should ideally show points along the diagonal line, where actual prices equal predicted prices. The data clustering along the line suggests a generally good model fit, but here we can observe the spread which indicates variance in predictions, especially at higher price points.

```{r}
## Visualization
gg <- ggplot(data=testData, aes(x=predicted_prices, y=price)) +
    geom_point() +
    geom_smooth(method="lm", col="blue") +
    labs(title="Actual vs Predicted Prices", x="Predicted Prices", y="Actual Prices")

# Convert ggplot to plotly
p <- ggplotly(gg, width = 600, height = 400)

# Show the interactive plot
p
```

The actual vs. predicted prices plot shows a similar trend, with a wider spread of points at higher price points, indicating that the model may not perform as well in predicting prices for more expensive properties.

### Linear Regression - Reducing Complexity

To solve this issue of variance at higher price points, we can filter the data to focus on a more specific range of canton. Specifically cantons valais, tessin, vaud, Berne, Fribourg to see if the model performs better within this range.

Indeed, as seen in the EDA section, these cantons have more properties and show similar price distributions.

Also, as seen in the previous linear regression model, we consider removing variables that show a high p-value and a wide confidence interval (CI) indicating less statistical significance and reliability.

Thus `number_of_rooms` can be removed as it has a p-value of 0.6, as 'floor' with a p-value of 0.3 indicating they are not statistically significant.
`Loft`, `Rustic house`, and `Terrace flat` also show high p-values (0.7 for `Loft` and `Rustic house`, 0.6 for `Terrace flat`) and wide confidence intervals, suggesting that they do not significantly impact the model and their removal would simplify the model without losing predictive power.
`Roof` flat with a p-value hovering around 0.10 (0.079 in the stepwise model) could be considered for removal if further simplification is needed, though its influence is closer to being significant compared to the others.
`Roof flat` and `Attic flat` will also be removed purely based on the eda where we saw that they have very few properties in the dataset.

```{r}
#select only 'Apartment', 'Single house', 'Villa', 'Attic flat', 'Farm house', 'Roof flat', 'Chalet', 'Duplex', 'Bifamiliar house' in property_type col
properties_filtered_2 <- properties_filtered %>% filter(property_type %in% c("Apartment", "Single house", "Villa", "Farm house", "Chalet", "Duplex", "Bifamiliar house"))
#select properties_filtered based on the canton valais, tessin, vaud, Berne, Fribourg
properties_filtered_2 <- properties_filtered_2 %>% filter(canton %in% c("Valais", "Ticino", "Vaud", "Bern", "Fribourg"))
```

#### Model Building and Evaluation
We then repeat the model building and evaluation process for this filtered dataset to compare the performance with the initial (best) model:
```{r}
# Model Building
## Split the Data
set.seed(123)  # for reproducibility
trainIndex <- createDataPartition(properties_filtered_2$price, p=0.8, list=FALSE)
trainData2 <- properties_filtered_2[trainIndex, ]
testData2 <- properties_filtered_2[-trainIndex, ]


## Fit the Model
lm_model_1.1 <- lm(price ~ square_meters + property_type +  year_category  + Political_cluster + Tax_cluster + Demographic_cluster, data=trainData2)

summary(lm_model_1.1)

# Model Evaluation
## Diagnostic Checks
#plot(lm_model)
#ad.test(residuals(lm_model))

#use gt summary to show the result
tbl_reg_1.1 <- gtsummary::tbl_regression(lm_model_1.1)

tbl_reg_1_vs_1.1 <- tbl_merge(
  tbls= list(tbl_reg_2, tbl_reg_1.1),
  tab_spanner = c("**Stepwise Model (All Prices)**", "**Basic Model (reduce complexity)**")
  )
tbl_reg_1_vs_1.1
```

- Consistency and Impact: 

    - Most variables retain their significance and exhibit similar or enhanced predictive power across both models, affirming their robustness as predictors.

- Shifts in Significance: 

    - Reducing model complexity has led to significant changes in the predictive impact and statistical significance of some variables like `Duplex` and the `1919-1945` (two world war period might explain that) year category, suggesting a potential overestimation or uncertainty in their effects when fewer variables are included

#### Assess Overfitting

```{r}
# For the Linear Model
lm_train_pred <- predict(lm_model_1.1, newdata = trainData2)
lm_test_pred <- predict(lm_model_1.1, newdata = testData2)

# Calculate RMSE and R-squared for Training Data
lm_train_rmse <- sqrt(mean((trainData2$price - lm_train_pred)^2))
lm_train_rsquared <- summary(lm(lm_train_pred ~ trainData2$price))$r.squared

# Calculate RMSE and R-squared for Test Data
lm_test_rmse <- sqrt(mean((testData2$price - lm_test_pred)^2))
lm_test_rsquared <- summary(lm(lm_test_pred ~ testData2$price))$r.squared

# show the results in a table
results_table <- data.frame(
  Model = c("Linear Regression"),
  RMSE_Train = lm_train_rmse,
  RMSE_Test = lm_test_rmse,
  Rsquared_Train = lm_train_rsquared,
  Rsquared_Test = lm_test_rsquared
)

#show table in html
kable(results_table, format = "html") %>%
  kable_styling(position = "center", bootstrap_options = c("striped", "bordered", "hover", "condensed"))
```

- Consistent Generalization: 

    - We observe a slight increase in RMSE from training to testing. This minor variation suggests consistent performance across both training and testing datasets, despite a minimal loss in prediction accuracy on new data.

- Unusual Predictive Performance: 

    - Interestingly, the model's R-squared is higher on the test set than on the training set, which is atypical as models generally perform better on training data. This anomaly might indicate that the test data contains features that align well with the model's parameters, or it may suggest a robustness in the model's ability to handle and explain variability in unseen data more effectively than in the training data.

##### Metrics 

Here are the performance metrics for the filtered:
```{r}
# print R-squared and Adj R-squared and RMSE and MAE and AIC
r_sq <- summary(lm_model_1.1)$r.squared
adj_r_sq <- summary(lm_model_1.1)$adj.r.squared
rmse <- rmse(testData2$price, predict(lm_model_1.1))
mae <- mae(testData2$price, predict(lm_model_1.1, newdata=testData2))
aic <- AIC(lm_model_1.1)
# show those metrics in a html table
metrics_1.1 <- kable(data.frame(r_sq, adj_r_sq, rmse, mae, aic), format = "html", col.names = c("R-Squared", "Adjusted R-Squared", "RMSE", "MAE", "AIC")) %>%
  kable_styling(position = "center", bootstrap_options = c("striped", "bordered", "hover", "condensed")) %>%
  add_header_above(c("Basic Model Performance Metrics (less comple)" = 5))  
metrics_1.1
``` 

Here was the previous metrics of our first Basic model (more complex)
```{r}
metrics_2
``` 

We observe a very slight increase in r-squared, a big drop in RMSE, and a other drop in AIC, indicating that the model may perform better in predicting property prices within this specific canton range.

#### Variable Selection
```{r}
# stepwise regression
lm_model_2.1 <- step(lm_model_1.1)
# plot(lm_model2)
# ad.test(residuals(lm_model2))

lm_model_2.1

#use gt summary to show the result 
tbl_reg_2.1 <- gtsummary::tbl_regression(lm_model_2.1)
tbl_reg_1.1_vs_2.1 <- tbl_merge(
  tbls= list(tbl_reg_1.1, tbl_reg_2.1),
  tab_spanner = c("**Basic Model**", "**Stepwise Model**")
  )
tbl_reg_1.1_vs_2.1
```

We observe no change. The stepwise model retains the same predictors as the initial model, indicating that the selected variables are the most relevant for predicting property prices within this specific canton range.

##### Metrics

Here are the performance metrics for the stepwise model with prices between the 10th and 90th percentiles as well as the comparison with the initial model:
```{r}
## Performance Metrics
r_sq <- summary(lm_model_2.1)$r.squared
adj_r_sq <- summary(lm_model_2.1)$adj.r.squared
rmse <- rmse(testData2$price, predict(lm_model_2.1))
mae <- mae(testData2$price, predict(lm_model_2.1, newdata=testData2))
aic <- AIC(lm_model_2.1)
# show those metrics in a html table
metrics_2.1 <- kable(data.frame(r_sq, adj_r_sq, rmse, mae, aic), format = "html", col.names = c("R-Squared", "Adjusted R-Squared", "RMSE", "MAE", "AIC")) %>%
  kable_styling(position = "center", bootstrap_options = c("striped", "bordered", "hover", "condensed")) %>%
  add_header_above(c("Stepwise Model Performance Metrics (Selected Canton)" = 5))
metrics_2.1
```

Here was the previous metrics of our Basic Model (without Stepwise Integration)

```{r}
metrics_1.1
```

As indicated before, the predictors remain consistent across both models, so the metrics are similar.

#### Cross-Validation
```{r}
## Cross-Validation
cv_results2 <- train(price ~ square_meters + year_category + property_type + Demographic_cluster +Political_cluster + Tax_cluster, data=trainData2, method="lm", trControl=trainControl(method="cv", number=10))
summary(cv_results2)

#show the CV result in the html
metrics_cv2 <- kable(cv_results2$results, format = "html") %>%
  
  kable_styling(position = "center", bootstrap_options = c("striped", "bordered", "hover", "condensed"))  %>%
  add_header_above(c("10 Fold Cross Validation Metrics (less complex)" = 7))
metrics_cv2
```

Here was the previous metrics of our first Basic Model (without selectig for Cantons) :
```{r}
metrics_cv_1
```


- Consistency and Predictive Stability: 

    - The less complex model shows more consistent performance across different data subsets, shown by lower variability in both RMSE and R-squared during cross-validation. It might offer more stable and reliable predictions compared to the more complex model.

- Explanatory Power and Accuracy: 

    - While the more complex model might score slightly better on average in predicting exact values, it's less consistent. This means in some cases it might not perform as expected. On the other hand, the less complex model, although not always as sharp, explains the variations in the data more steadily across different tests.

#### Model testing
##### Residual vs Predicted Prices
```{r}
# Model Testing 
## Test the Model
predicted_prices <- predict(lm_model_2.1, newdata=testData2)
testData2$predicted_prices <- predicted_prices  # Add to testData to ensure alignment
# Calculate residuals
testData2$test_residuals <- testData2$price - predicted_prices  # Manually compute residuals

# Residual Analysis
gg <- ggplot(data = testData2, aes(x = predicted_prices, y = test_residuals)) +
  geom_point() +
  geom_smooth(method = "lm", color = "blue") +
  labs(title = "Residuals vs Predicted Prices", x = "Predicted Prices", y = "Residuals")

# Convert ggplot to plotly
p <- ggplotly(gg, width = 600, height = 400)

# Show the interactive plot
p
```

The residual plot for the less compley modeldisplays a similar pattern where a clustering of residuals around the zero line and some extreme outliers.

##### Actual vs Predicted Prices
```{r}
## Visualization
gg <- ggplot(data=testData, aes(x=predicted_prices, y=price)) +
    geom_point() +
    geom_smooth(method="lm", col="blue") +
    labs(title="Actual vs Predicted Prices", x="Predicted Prices", y="Actual Prices")

# Convert ggplot to plotly
p <- ggplotly(gg, width = 600, height = 400)

# Show the interactive plot
p
```

The same trend is observed in the actual vs. predicted prices plot, with a wider spread of points at higher price points, indicating that the model may not perform as well in predicting prices for more expensive properties.

### Conclusion

For our first model, we used a linear regression model to predict property prices based on various features. We evaluated the model using performance metrics such as R-squared, RMSE, and MAE to assess its predictive accuracy and explanatory power. The model does not perform well even if reducing the complexity. Especially at higher price points. We have tried to limit the price range to only include prices lower than 3.3 Million (as per the EDA) but the model was even worse. 

Thus we have to consider other models to predict the property prices.

## Model 2

This section provides a comprehensive outline of the linear regression model and analysis methods employed in the study of property price determinants.

### Tools

The Random Forest model was implemented using R with the `randomForest` package, which facilitated the model fitting and evaluation. Data partitioning was managed by `caret`, ensuring robust training and testing sets. Performance metrics were derived using base R functions and visualizations of variable importance were generated using `varImpPlot` from the randomForest package. Hyperparameter tuning was conducted to optimize the model, enhancing its predictive accuracy and generalizability.

### Random Forest

As an alternative, the Random Forest model addressed non-linear relationships using multiple decision trees, enhancing prediction accuracy and robustness. After data splitting and hyperparameter tuning, predictor importance was assessed, and model stability confirmed with performance metrics like R-squared, RMSE, and MAE through cross-validation.

#### Model Building and Evaluation

We split the data into training and testing sets, fit the Random Forest model and then evaluated the model using performance metrics such as R-squared, RMSE, and MAE to assess its predictive accuracy and explanatory power.
```{r}
#split the data in training and test set 0.8/0.2
set.seed(123)  # for reproducibility
#trainIndex <- createDataPartition(properties_filtered$price, p=0.8, list=FALSE)
#trainData <- properties_filtered[trainIndex, ]
#testData <- properties_filtered[-trainIndex, ]


#apply the RF model as a regression
rf_model <- randomForest(price ~., data=trainData, ntree=500, importance=TRUE)
rf_model.pred_rf <- predict(rf_model, newdata=testData)


rmse_rf <- sqrt(mean((testData$price - rf_model.pred_rf)^2))
mae_rf <- mean(abs(testData$price - rf_model.pred_rf))
r_sq_rf <- cor(testData$price, rf_model.pred_rf)^2

# show those metrics in a html table
metrics_rf <- kable(data.frame(r_sq_rf, rmse_rf, mae_rf), format = "html", col.names = c("R-Squared", "RMSE", "MAE")) %>%
  kable_styling(position = "center", bootstrap_options = c("striped", "bordered", "hover", "condensed")) %>%
  add_header_above(c("Random Forest Model Performance Metrics" = 3))
metrics_rf
```

```{r}
# Predictions on Training Data
rf_train_pred <- predict(rf_model, newdata=trainData)

# Calculate performance metrics for Training Data
rmse_train <- sqrt(mean((trainData$price - rf_train_pred)^2))
mae_train <- mean(abs(trainData$price - rf_train_pred))
r_sq_train <- cor(trainData$price, rf_train_pred)^2

# Existing metrics for Test Data
# These were already calculated: rmse_rf, mae_rf, r_sq_rf using rf_model.pred_rf

# Create a data frame for displaying metrics comparison
performance_metrics <- data.frame(
  DataSet = c("Training", "Testing"),
  RMSE = c(rmse_train, rmse_rf),
  MAE = c(mae_train, mae_rf),
  R_Squared = c(r_sq_train, r_sq_rf)
)

# Generate HTML table using kable and apply styling
kable(performance_metrics, format = "html", col.names = c("Data Set", "RMSE", "MAE", "R-Squared")) %>%
  kable_styling(position = "center", bootstrap_options = c("striped", "bordered", "hover", "condensed")) %>%
  add_header_above(c("Random Forest Model Performance Metrics" = 4))
```


##### Evaluation Metrics and Comparison
Comparing with the best model of the linear regression, we can see that the Random Forest model has a higher R-squared value and lower RMSE and MAE values, indicating better predictive accuracy.
```{r}
metrics_2
```

The plot shows the actual vs. predicted prices, with the diagonal line indicating perfect predictions.
```{r}
plot(testData$price ~rf_model.pred_rf, col = 'skyblue',
     xlab = 'Actual Price', ylab = 'Predicted Price',
     main = 'Actual vs Predicted Price')

abline(0,1, col = 'darkred')
```


#### Variable Importance
##### Importance of Predictors
VI plots are a useful tool to understand the relative importance of predictors in the Random Forest model. This plot shows the importance of each predictor in the model, helping to identify key features that drive price predictions.
```{r eval=TRUE}
varImpPlot(rf_model)
```
We see that square_meters is the most important predictor.

##### Interpretation of Key Features
```{r}
# Get the importance of each feature
importance(rf_model)
```


#### Cross-Validation

A SUPPRIMER TOO LONG OU TENTER OTHER APPROACH
```{r eval=TRUE}
# cv_results_rf <- train(price ~., data=trainData, method="rf", trControl=trainControl(method="cv", number=5))
# summary(cv_results_rf)
# 
# #show the CV result in the html
# metrics_cv_rf <- kable(cv_results_rf$results, format = "html") %>%
#   kable_styling(position = "center", bootstrap_options = c("striped", "bordered", "hover", "condensed"))  %>%
#   add_header_above(c("10 Fold Cross Validation Metrics (Random Forest)" = 7))
# metrics_cv_rf
```

EVALUATE MODEL STABILITY

#### Hyperparameter Tuning
##### Tuning Hyperparameters (mtry, ntree)

A SUPPRIMER TOO LONG OU TENTER OTHER APPROACH
```{r eval=TRUE}
tuneGrid <- expand.grid(mtry = seq(2, sqrt(ncol(trainData)), by = 1))  # Tune over a range of mtry values

# Train the model with tuning
rf_tuned <- train(price ~ number_of_rooms + square_meters + canton + floor + year_category + Demographic_cluster + Political_cluster + Tax_cluster, 
                  data = trainData, method = "rf", 
                  trControl = trainControl(method = "cv", number = 5, search = "grid"), 
                  tuneGrid = tuneGrid, 
                  ntree = 500)

# Plotting the tuning effect
plot(rf_tuned)

# Now, evaluate the tuned model
rf_model_pred <- predict(rf_tuned, newdata = testData)

# Calculate performance metrics
rmse_rf <- sqrt(mean((testData$price - rf_model_pred)^2))
mae_rf <- mean(abs(testData$price - rf_model_pred))
r_sq_rf <- cor(testData$price, rf_model_pred)^2

# Show metrics
metrics_rf <- kable(data.frame(R_Squared = r_sq_rf, RMSE = rmse_rf, MAE = mae_rf),
                    format = "html", col.names = c("R-Squared", "RMSE", "MAE")) %>%
              kable_styling(position = "center", bootstrap_options = c("striped", "bordered", "hover", "condensed")) %>%
              add_header_above(c("Tuned Random Forest Model Performance Metrics" = 3))
metrics_rf
```

##### Grid Search Methodology
```{r}
# Grid search methodology
```   

##### Evaluating Tuned Model Performance
```{r}

```


### Random Forest 2 - less complex

```{r}
#filter price on less than 3.3M and year 2016-2024 and by canton
properties_filtered_3 <- properties_filtered_2 %>% filter(price < 3300000 & year_category =="2016-2024" & canton == "Vaud")
# Model Building
## Split the Data and randomize it
set.seed(123)  # for reproducibility

trainIndex <- createDataPartition(properties_filtered_3$price, p=0.8, list=FALSE)
trainData3 <- properties_filtered_3[trainIndex, ]
testData3 <- properties_filtered_3[-trainIndex, ]

#remove canton and demographic clusters

#apply the RF model as a regression
rf_model <- randomForest(price ~ square_meters + number_of_rooms + property_type  + Political_cluster +Tax_cluster, data=testData3, ntree=500, importance=TRUE)
rf_model.pred_rf <- predict(rf_model, newdata=testData3)


rmse_rf <- sqrt(mean((testData3$price - rf_model.pred_rf)^2))
mae_rf <- mean(abs(testData2$price - rf_model.pred_rf))
r_sq_rf <- cor(testData3$price, rf_model.pred_rf)^2

# show those metrics in a html table
metrics_rf_2 <- kable(data.frame(r_sq_rf, rmse_rf, mae_rf), format = "html", col.names = c("R-Squared", "RMSE", "MAE")) %>%
  kable_styling(position = "center", bootstrap_options = c("striped", "bordered", "hover", "condensed")) %>%
  add_header_above(c("Random Forest Model Metrics (less complex)" = 3))
metrics_rf_2
```

#### Assess Overfitting

```{r}
#assess overfitting
# For the Random Forest Model
rf_train_pred <- predict(rf_model, newdata = trainData3)
rf_test_pred <- predict(rf_model, newdata = testData3)

# Calculate RMSE and R-squared and Mae for Training Data
rf_train_rmse <- sqrt(mean((trainData3$price - rf_train_pred)^2))
rf_train_rsquared <- cor(trainData3$price, rf_train_pred)^2
rf_train_mae <- mean(abs(trainData3$price - rf_train_pred))

# Calculate RMSE and R-squared for Test Data
rf_test_rmse <- sqrt(mean((testData3$price - rf_test_pred)^2))
rf_test_rsquared <- cor(testData3$price, rf_test_pred)^2
rf_test_mae <- mean(abs(testData3$price - rf_test_pred))


# Create a data frame for displaying metrics comparison
performance_metrics <- data.frame(
  Model = c("Random Forest"),
  RMSE_Train = rf_train_rmse,
  RMSE_Test = rf_test_rmse,
  Rsquared_Train = rf_train_rsquared,
  Rsquared_Test = rf_test_rsquared,
  MAE_Train = rf_train_mae,
  MAE_Test = rf_test_mae
)
# Generate HTML table using kable and apply styling
kable(performance_metrics, format = "html", col.names = c("Model", "RMSE Train", "RMSE Test", "R-Squared Train", "R-Squared Test", "MAE Train", "MAE Test")) %>%
  kable_styling(position = "center", bootstrap_options = c("striped", "bordered", "hover", "condensed"))
```


```{r}
plot(testData3$price ~rf_model.pred_rf, col = 'skyblue',
     xlab = 'Actual Price', ylab = 'Predicted Price',
     main = 'Actual vs Predicted Price')

abline(0,1, col = 'darkred')
```


#### Cross-Validation
```{r}
## Cross-Validation
cv_results3 <- train(price ~ square_meters + year_category + property_type + Demographic_cluster +Political_cluster + Tax_cluster, data=trainData3, method="lm", trControl=trainControl(method="cv", number=10))
summary(cv_results2)

#show the CV result in the html
metrics_cv3 <- kable(cv_results3$results, format = "html") %>%
  
  kable_styling(position = "center", bootstrap_options = c("striped", "bordered", "hover", "condensed"))  %>%
  add_header_above(c("10 Fold Cross Validation Metrics (less complex)" = 7))
metrics_cv3
```
