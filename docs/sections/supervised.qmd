# Supervised learning

* Data splitting (if a training/test set split is enough for the global analysis, at least one CV or bootstrap must be used)
* Two or more models 
* Two or more scores
* Tuning of one or more hyperparameters per model
* Interpretation of the model(s)

## Model 1
This section provides a comprehensive outline of the linear regression model and analysis methods employed in the study of property price determinants.

### Tools and Software
The study was conducted using R, a programming language and environment widely recognized for its robust capabilities in statistical computing and graphics. Key packages used include:

- `corrplot` for visualization of correlation matrices, aiding in preliminary feature selection.
car for diagnostic testing and variance inflation factor (VIF) analysis to detect multicollinearity among predictors.
- `caret` for creating training and testing sets, and managing cross-validation processes.
- `ggplot2` and `plotly` for creating visual representations of model diagnostics, predictions, and residuals.
- `gtsummary` for creating publication-ready tables summarizing regression analysis results.

Each of these tools was chosen for its specific functionality that aids in data analysis, ensuring that each step of the model building and evaluation process is well-supported.

### Linear Regression - With All Prices
#### Correlation Analysis - Continuous Variable

Initially, a correlation analysis was conducted to identify and visualize linear relationships between the property prices and potential predictive variables such as the number of rooms and square meters. The correlation matrix was computed and plotted using the `corrplot` package:

```{r}
correlation_matrix <- cor(properties_filtered[ , c("price", "number_of_rooms", "square_meters")], use="complete.obs")
corrplot(correlation_matrix, method="square", type="upper", tl.col="black", tl.srt=45, tl.cex=0.8, cl.cex=0.8, addCoef.col="black", number.cex=0.8, order="hclust", hclust.method="complete", tl.pos="lt", tl.offset=0.5, cl.pos="n", cl.length=1.5)
```

- We can observe that the correlation between the number of rooms and price (0.46) and square meters and price (0.67) suggests a moderate correlation with the number of rooms and a strong correlation with square meters. 
- The number of rooms and square meters also have a strong correlation (0.69), indicating potential multicollinearity between these predictors.

- **Question : How to make the correlation with `categorical` variables?**
- **Question : Is VIF analysis really necessary and meaningful ?**
#### GVIF (Generalized Variance Inflation Factor)
```{r}
properties_filtered
## Multicollinearity Check
model_for_vif <- lm(price ~ number_of_rooms + square_meters + canton + floor + year_category + Demographic_cluster + Political_cluster + Tax_cluster, data=properties_filtered)
vif_values <- vif(model_for_vif)
#show the result in the html
kable(vif_values, format = "html") %>%
  kable_styling(full_width = F)
```

 - High VIF Values:
canton and Tax_cluster have VIF values much greater than 10, indicating serious multicollinearity issues. These predictors are highly correlated with other predictors in the model.

- Moderate VIF Values:
Other predictors like number_of_rooms, square_meters, Political_cluster, etc., have VIF values below 5, indicating acceptable multicollinearity.

We removed the `canton` variable from the model due to its high VIF value, which could lead to unstable coefficient estimates and unreliable model predictions.

We keep the `Tax_cluster` variable in the model for now, as it may provide valuable information for predicting property prices.

####Basic Model
##### Model Building and Evaluation

The data set was split into training and testing sets to validate the model’s performance. The linear regression model was then fitted using selected predictors:
```{r}
# Model Building
## Split the Data
set.seed(123)  # for reproducibility
trainIndex <- createDataPartition(properties_filtered$price, p=0.8, list=FALSE)
trainData <- properties_filtered[trainIndex, ]
testData <- properties_filtered[-trainIndex, ]

## Fit the Model
lm_model <- lm(price ~ number_of_rooms + square_meters + property_type + floor + year_category  + Demographic_cluster +Political_cluster + Tax_cluster , data=trainData)

summary(lm_model)
```

Diagnostic checks such as residual analysis and normality tests were conducted to validate model assumptions. Performance metrics including R-squared and RMSE were calculated to assess the model's explanatory power and prediction accuracy.

```{r}
# Model Evaluation
## Diagnostic Checks
#plot(lm_model)
#ad.test(residuals(lm_model))

#use gt summary to show the result
tbl_reg_1 <- gtsummary::tbl_regression(lm_model)
tbl_reg_1
```

- Significant Predictors of Price:

    - Square meters: The most influential variable with a strong positive effect on price.
    - Property types: Some types significantly affect prices, with villas and attic flats increasing prices, while single houses and farm houses decrease prices.
    - Year category: Newer properties consistently have higher prices, with significant positive impacts for all categories compared to the baseline.

##### Assess Overfitting

```{r}
# For the Linear Model
lm_train_pred <- predict(lm_model, newdata = trainData)
lm_test_pred <- predict(lm_model, newdata = testData)

# Calculate RMSE and R-squared for Training Data
lm_train_rmse <- sqrt(mean((trainData$price - lm_train_pred)^2))
lm_train_rsquared <- summary(lm(lm_train_pred ~ trainData$price))$r.squared

# Calculate RMSE and R-squared for Test Data
lm_test_rmse <- sqrt(mean((testData$price - lm_test_pred)^2))
lm_test_rsquared <- summary(lm(lm_test_pred ~ testData$price))$r.squared

# show the results in a table
results_table <- data.frame(
  Model = c("Linear Regression"),
  RMSE_Train = lm_train_rmse,
  RMSE_Test = lm_test_rmse,
  Rsquared_Train = lm_train_rsquared,
  Rsquared_Test = lm_test_rsquared
)

#show table in html
kable(results_table, format = "html") %>%
  kable_styling(position = "center", bootstrap_options = c("striped", "bordered", "hover", "condensed"))
```

No overfitting is observed as the RMSE and R-squared values are similar between the training and test sets, indicating that the model generalizes well to new data.

##### Metrics 

Here are the performance metrics for the initial model:
```{r}
# print R-squared and Adj R-squared and RMSE and MAE and AIC
r_sq <- summary(lm_model)$r.squared
adj_r_sq <- summary(lm_model)$adj.r.squared
rmse <- rmse(testData$price, predict(lm_model, newdata=testData))
mae <- mae(testData$price, predict(lm_model, newdata=testData))
aic <- AIC(lm_model)
# show those metrics in a html table
metrics_1 <- kable(data.frame(r_sq, adj_r_sq, rmse, mae, aic), format = "html", col.names = c("R-Squared", "Adjusted R-Squared", "RMSE", "MAE", "AIC")) %>%
  kable_styling(position = "center", bootstrap_options = c("striped", "bordered", "hover", "condensed"))  %>%
  add_header_above(c("Basic Model Performance Metrics" = 5)) 
metrics_1
```

- The model has moderate explanatory power, with an R-Squared of 0.493 and an Adjusted R-Squared of 0.492.

- Prediction accuracy, as indicated by RMSE (980,302) and MAE (491,715), shows that the model's predictions are reasonably close to actual prices but could be improved.

- The AIC value (500,701) provides a benchmark for comparing with other models

#### Hyperparameter Tuning
##### Stepwise Regression
Stepwise regression was performed to refine the model and improve its predictive performance. The resulting model was evaluated using the same diagnostic checks and performance metrics as the initial model:

```{r}
# stepwise regression
lm_model2 <- step(lm_model)

#use gt summary to show the result 
tbl_reg_2 <- gtsummary::tbl_regression(lm_model2)
tbl_reg_3 <- tbl_merge(
  tbls= list(tbl_reg_1, tbl_reg_2),
  tab_spanner = c("**Basic Model**", "**Model Stepwise**")
  )
tbl_reg_3
```

We observe :

- Consistency in Key Predictors: The primary significant predictors (e.g., square_meters, certain property types, and year categories) remain consistent across both models, indicating their robust influence on property prices.
- Similar Effect Sizes: The effect sizes (β) and confidence intervals for significant predictors are similar in both models, reinforcing the reliability of these predictors.

##### Lasso and Ridge Regression
A Lasso and Ridge regression were also performed to compare the performance of the linear regression model with regularization techniques. 
We fit both models using cross-validation to determine the optimal lambda (penalty parameter).
The plots show the lambda selection process for both Lasso and Ridge models.
```{r}
# Convert data frames to matrices for glmnet
dat_tr_re_mat_x <- as.matrix(trainData[, c("number_of_rooms", "square_meters", "floor", "year_category", "Demographic_cluster", "Political_cluster", "Tax_cluster")])
dat_tr_re_mat_y <- trainData$price

dat_te_re_mat_x <- as.matrix(testData[, c("number_of_rooms", "square_meters", "floor", "year_category", "Demographic_cluster", "Political_cluster", "Tax_cluster")])
dat_te_re_mat_y <- testData$price

#fit lasso and ridge
set.seed(123)  # For reproducibility

# Fit Lasso model
lasso_model <- cv.glmnet(dat_tr_re_mat_x, dat_tr_re_mat_y, alpha = 1) # Lasso

# Fit Ridge model
ridge_model <- cv.glmnet(dat_tr_re_mat_x, dat_tr_re_mat_y, alpha = 0) # Ridge

ridge_fit_best <- glmnet(x=dat_tr_re_mat_x, y = dat_tr_re_mat_y, 
                         lambda = ridge_model$lambda.min)

lasso_fit_best <- glmnet(x=dat_tr_re_mat_x, y=dat_tr_re_mat_y, 
                         lambda = lasso_model$lambda.min) #can also use lasso_fit$lambda.1se

# lasso & ridge performance on the training set
postResample(predict(ridge_fit_best, newx = dat_tr_re_mat_x), dat_tr_re_mat_y)
postResample(predict(lasso_fit_best, newx = dat_tr_re_mat_x), dat_tr_re_mat_y)
# lasso & ridge performance on the test set
postResample(predict(ridge_fit_best, newx = dat_te_re_mat_x), dat_te_re_mat_y)
postResample(predict(lasso_fit_best, newx = dat_te_re_mat_x), dat_te_re_mat_y)

# Step-wise lm performance on training and test sets
postResample(predict(lm_model2,trainData), dat_tr_re_mat_y)
postResample(predict(lm_model2,testData), dat_te_re_mat_y)
```
We then compared the performance of the Lasso and Ridge models with the stepwise linear regression model using RMSE and MAE:

```{r}
# Calculate RMSE and MAE
get_metrics <- function(predictions, actuals) {
  RMSE <- sqrt(mean((predictions - actuals)^2))
  MAE <- mean(abs(predictions - actuals))
  Rsquared <- cor(predictions, actuals)^2

  
  return(c(RMSE = RMSE, MAE = MAE, Rsquared = Rsquared) )
}

# Capture the performance metrics
ridge_train_preds <- predict(ridge_fit_best, newx = dat_tr_re_mat_x)
lasso_train_preds <- predict(lasso_fit_best, newx = dat_tr_re_mat_x)
ridge_test_preds <- predict(ridge_fit_best, newx = dat_te_re_mat_x)
lasso_test_preds <- predict(lasso_fit_best, newx = dat_te_re_mat_x)
lm_train_preds <- predict(lm_model2, trainData)
lm_test_preds <- predict(lm_model2, testData)

ridge_train_metrics <- get_metrics(ridge_train_preds, dat_tr_re_mat_y)
lasso_train_metrics <- get_metrics(lasso_train_preds, dat_tr_re_mat_y)
ridge_test_metrics <- get_metrics(ridge_test_preds, dat_te_re_mat_y)
lasso_test_metrics <- get_metrics(lasso_test_preds, dat_te_re_mat_y)
lm_train_metrics <- get_metrics(lm_train_preds, dat_tr_re_mat_y)
lm_test_metrics <- get_metrics(lm_test_preds, dat_te_re_mat_y)

# Create a data frame with the performance metrics
performance_df <- data.frame(
  Model = c("Ridge (Training)", "Lasso (Training)", "Ridge (Test)", "Lasso (Test)", "Stepwise (Training)", "Stepwise (Test)"),
  RMSE = c(ridge_train_metrics["RMSE"], lasso_train_metrics["RMSE"], ridge_test_metrics["RMSE"], lasso_test_metrics["RMSE"], lm_train_metrics["RMSE"], lm_test_metrics["RMSE"]),
  MAE = c(ridge_train_metrics["MAE"], lasso_train_metrics["MAE"], ridge_test_metrics["MAE"], lasso_test_metrics["MAE"], lm_train_metrics["MAE"], lm_test_metrics["MAE"]),
  Rsquared = c(ridge_train_metrics["Rsquared"], lasso_train_metrics["Rsquared"], ridge_test_metrics["Rsquared"], lasso_test_metrics["Rsquared"], lm_train_metrics["Rsquared"], lm_test_metrics["Rsquared"])
)

# Create the kable extra table
performance_table <- kable(performance_df, format = "html") %>%
  kable_styling(full_width = FALSE, position = "center", bootstrap_options = c("striped", "bordered", "hover", "condensed")) %>%
  add_header_above(c( "Performance Metrics (RMSE, MAE, R-sq)" = 4))

# Print the table
performance_table
```

The Stepwise model is the preferred choice for predicting property prices based on the provided metrics. It offers the best balance of accuracy and predictive power. Lasso regression is a good alternative, particularly if model simplicity and interpretability are priorities due to its ability to shrink coefficients and eliminate irrelevant features. Ridge regression, while slightly less accurate, provides stable performance and handles multicollinearity well, good for our tax_cluster variable with high VIF.

##### Metrics

Here we compare the performance metrics of the initial model and the stepwise model.
The metrics of our initial model :
```{r}
metrics_1
```

Stepwise model:
```{r}
# print R-squared and Adj R-squared and RMSE and MAE and AIC
r_sq <- summary(lm_model2)$r.squared
adj_r_sq <- summary(lm_model2)$adj.r.squared
rmse <- rmse(testData$price, predict(lm_model2, newdata=testData))
mae <- mae(testData$price, predict(lm_model2, newdata=testData))
aic <- AIC(lm_model2)
# show those metrics in a html table
metrics_2 <- kable(data.frame(r_sq, adj_r_sq, rmse, mae, aic), format = "html", col.names = c("R-Squared", "Adjusted R-Squared", "RMSE", "MAE", "AIC")) %>%
  kable_styling(position = "center", bootstrap_options = c("striped", "bordered", "hover", "condensed"))  %>%
  add_header_above(c("Stepwise Model Performance Metrics" = 5)) 
metrics_2
```

The Stepwise model offers a slight improvement over the Basic model in terms of prediction accuracy (lower MAE) and model efficiency (lower AIC).
Both models perform similarly in explaining the variance in property prices, with nearly identical R-Squared and Adjusted R-Squared values.
Given the minimal differences, the choice between models may depend on the preference for model simplicity (Stepwise model) versus a more comprehensive approach (Basic model).

#### Cross-Validation

Cross-validation was used to assess the model's robustness, typically to avoid overfitting and ensure that the model generalizes well to new data., using the `caret` package to manage this process efficiently. The CV results show metrics like RMSE and R-squared across folds, which indicate the model’s performance stability across different subsets of the data.

10-fold cross-validation Metrics:
```{r}
#add + Demographic_cluster +Political_cluster + Tax_cluster after dealing with NAN
## Cross-Validation
cv_results <- train(price ~ number_of_rooms + square_meters + year_category + property_type , data=trainData, method="lm", trControl=trainControl(method="cv", number=10))
summary(cv_results)


#show the CV result in the html
metrics_cv_1 <- kable(cv_results$results, format = "html") %>%
  kable_styling(position = "center", bootstrap_options = c("striped", "bordered", "hover", "condensed"))  %>%
  add_header_above(c("10 Fold Cross Validation Metrics" = 7))
metrics_cv_1
```

Here are the performance metrics for the stepwise model:
```{r}
metrics_2
```

- Better Prediction Accuracy: 10-fold cross validation shows a lower RMSE compared to both Basic and Stepwise models.
- Slightly Lower Explanatory Power: R-Squared is marginally lower in the cross-validated model.

We can thus say that the model generalizes well to new data, with improved prediction accuracy compared to the initial models.

#### Model testing

We chose the stepwise model as the best model for the linear regresion due to its balance of accuracy and simplicity.

The final model was tested using the unseen test dataset to evaluate its predictive accuracy. Residual plots and actual vs. predicted price plots were created to visually assess model performance:

##### Residual vs Predicted Prices
This plot  shows residuals (differences between observed and predicted prices) against predicted prices. Ideally, residuals should randomly scatter around the horizontal line at zero, indicating that the model doesn’t systematically overestimate or underestimate prices. 

```{r}
# Model Testing 
## Test the Model
predicted_prices <- predict(lm_model2, newdata=testData)
testData$predicted_prices <- predicted_prices  # Add to testData to ensure alignment
# Calculate residuals
testData$test_residuals <- testData$price - predicted_prices  # Manually compute residuals

# Residual Analysis
gg <- ggplot(data = testData, aes(x = predicted_prices, y = test_residuals)) +
  geom_point() +
  geom_smooth(method = "lm", color = "blue") +
  labs(title = "Residuals vs Predicted Prices", x = "Predicted Prices", y = "Residuals")

# Convert ggplot to plotly
p <- ggplotly(gg, width = 600, height = 400)

# Show the interactive plot
p
```

As we can observe, the spread of residuals suggests potential issues with model fit, particularly for higher price predictions where the variance seems to increase.

##### Actual vs Predicted Prices

This plot should ideally show points along the diagonal line, where actual prices equal predicted prices. The data clustering along the line suggests a generally good model fit, but here we can observe the spread which indicates variance in predictions, especially at higher price points.

```{r}
## Visualization
gg <- ggplot(data=testData, aes(x=predicted_prices, y=price)) +
    geom_point() +
    geom_smooth(method="lm", col="blue") +
    labs(title="Actual vs Predicted Prices", x="Predicted Prices", y="Actual Prices")

# Convert ggplot to plotly
p <- ggplotly(gg, width = 600, height = 400)

# Show the interactive plot
p
```

We also observe a spread in the actual vs. predicted prices plot, indicating that the model may not capture the variance in property prices effectively, particularly at higher price points.

### Linear Regression for Specific Canton

To solve this issue of variance at higher price points, we can filter the data to focus on a more specific range of canton. Specifically cantons valais, tessin, vaud, Berne, Fribourg to see if the model performs better within this range.

Indeed, as seen in the EDA section, these cantons have more porperties and show similar price distributions.

```{r}
#selectt properties_filtered based on the canton valais, tessin, vaud, Berne, Fribourg
properties_filtered_ <- properties_filtered %>% filter(canton %in% c("Valais", "Ticino", "Vaud", "Bern", "Fribourg"))
#show each unique value in canton col
unique(properties_filtered_$canton)
```

#### Model Building and Evaluation
We then repeat the model building and evaluation process for this filtered dataset to compare the performance with the initial (best) model:
```{r}
# Model Building
## Split the Data
set.seed(123)  # for reproducibility
trainIndex <- createDataPartition(properties_filtered$price, p=0.8, list=FALSE)
trainData <- properties_filtered[trainIndex, ]
testData <- properties_filtered[-trainIndex, ]

## Fit the Model
lm_model_10_90 <- lm(price ~ number_of_rooms + square_meters + property_type + floor + year_category  + Political_cluster + Tax_cluster + Demographic_cluster, data=trainData)

summary(lm_model_10_90)

# Model Evaluation
## Diagnostic Checks
#plot(lm_model)
#ad.test(residuals(lm_model))

#use gt summary to show the result
tbl_reg_1_10_90 <- gtsummary::tbl_regression(lm_model_10_90)

tbl_reg_3_vs_10_90 <- tbl_merge(
  tbls= list(tbl_reg_3, tbl_reg_1_10_90),
  tab_spanner = c("**Stepwise Model (All Prices)**", "**Basic Model (10-90 Qt)**")
  )
tbl_reg_3_vs_10_90
```

##### Metrics 

Here are the performance metrics for the model with prices between the 10th and 90th percentiles:
```{r}
# print R-squared and Adj R-squared and RMSE and MAE and AIC
r_sq <- summary(lm_model_10_90)$r.squared
adj_r_sq <- summary(lm_model_10_90)$adj.r.squared
rmse <- rmse(testData$price, predict(lm_model_10_90))
mae <- mae(testData$price, predict(lm_model_10_90, newdata=testData))
aic <- AIC(lm_model_10_90)
# show those metrics in a html table
metrics_1_10_90 <- kable(data.frame(r_sq, adj_r_sq, rmse, mae, aic), format = "html", col.names = c("R-Squared", "Adjusted R-Squared", "RMSE", "MAE", "AIC")) %>%
  kable_styling(position = "center", bootstrap_options = c("striped", "bordered", "hover", "condensed")) %>%
  add_header_above(c("Basic Model Performance Metrics (Selected Canton)" = 5))  
metrics_1_10_90
``` 

Here was the previous metrics of our first Basic model (without the 10-90 Qt filter)
```{r}
metrics_2
``` 

We observe a sgifnicant increase in  RMSE for the selected canton model which indicates that the model may not perform as well in predicting property prices within this specific canton range.

#### Variable Selection
```{r}
# stepwise regression
lm_model2_10_90 <- step(lm_model_10_90)
# plot(lm_model2)
# ad.test(residuals(lm_model2))

lm_model2_10_90

#use gt summary to show the result 
tbl_reg_2_10_90 <- gtsummary::tbl_regression(lm_model2_10_90)
tbl_reg_3_10_90 <- tbl_merge(
  tbls= list(tbl_reg_1_10_90, tbl_reg_2_10_90),
  tab_spanner = c("**Basic Model**", "**Stepwise Model**")
  )
tbl_reg_3_10_90
```

We observe that number_of_rooms and floor variable are dropeed from the model, indicating that these variables may not be significant predictors of property prices within this specific canton range.

##### Metrics

Here are the performance metrics for the stepwise model with prices between the 10th and 90th percentiles as well as the comparison with the initial model:
```{r}
## Performance Metrics
r_sq <- summary(lm_model2_10_90)$r.squared
adj_r_sq <- summary(lm_model2_10_90)$adj.r.squared
rmse <- rmse(testData$price, predict(lm_model2_10_90))
mae <- mae(testData$price, predict(lm_model2_10_90, newdata=testData))
aic <- AIC(lm_model2_10_90)
# show those metrics in a html table
metrics_2_10_90 <- kable(data.frame(r_sq, adj_r_sq, rmse, mae, aic), format = "html", col.names = c("R-Squared", "Adjusted R-Squared", "RMSE", "MAE", "AIC")) %>%
  kable_styling(position = "center", bootstrap_options = c("striped", "bordered", "hover", "condensed")) %>%
  add_header_above(c("Stepwise Model Performance Metrics (Selected Canton)" = 5))
metrics_2_10_90
```

Here was the previous metrics of our Basic Model (without Stepwise Integration)

```{r}
metrics_1_10_90
```

Here was the previous metrics of our stepwise model (without the 10-90 Qt filter)
```{r}
metrics_2
```

We observe that selected model is worse than the initial model in terms of prediction accuracy (higher RMSE) and all other metrics are more or less the same. This means that the model may not perform as well in predicting property prices within this specific canton range. The higher RMSE indicates that the model's predictions are further from the actual prices, suggesting that the model may not generalize well to this specific canton range.

#### Cross-Validation
```{r}
## Cross-Validation
cv_results2 <- train(price ~ number_of_rooms + square_meters + year_category + property_type, data=trainData, method="lm", trControl=trainControl(method="cv", number=10))
summary(cv_results2)

#show the CV result in the html
metrics_cv2 <- kable(cv_results2$results, format = "html") %>%
  
  kable_styling(position = "center", bootstrap_options = c("striped", "bordered", "hover", "condensed"))  %>%
  add_header_above(c("10 Fold Cross Validation Metrics (10-90th Qt)" = 7))
metrics_cv2
```

Here was the previous metrics of our first Basic Model (without the 10-90 Qt filter)
```{r}
metrics_cv_1
```

#### Model testing
##### Residual vs Predicted Prices
```{r}
# Model Testing 
## Test the Model
predicted_prices <- predict(lm_model2_10_90, newdata=testData)
testData$predicted_prices <- predicted_prices  # Add to testData to ensure alignment
# Calculate residuals
testData$test_residuals <- testData$price - predicted_prices  # Manually compute residuals

# Residual Analysis
gg <- ggplot(data = testData, aes(x = predicted_prices, y = test_residuals)) +
  geom_point() +
  geom_smooth(method = "lm", color = "blue") +
  labs(title = "Residuals vs Predicted Prices", x = "Predicted Prices", y = "Residuals")

# Convert ggplot to plotly
p <- ggplotly(gg, width = 600, height = 400)

# Show the interactive plot
p
```

##### Actual vs Predicted Prices
```{r}
## Visualization
gg <- ggplot(data=testData, aes(x=predicted_prices, y=price)) +
    geom_point() +
    geom_smooth(method="lm", col="blue") +
    labs(title="Actual vs Predicted Prices", x="Predicted Prices", y="Actual Prices")

# Convert ggplot to plotly
p <- ggplotly(gg, width = 600, height = 400)

# Show the interactive plot
p
```

## Model 2
### Random Forest

We decided to implement a Random Forest model to compare its performance with the linear regression model. Random Forest is an ensemble learning method that builds multiple decision trees during training and outputs the mode of the classes or the mean prediction of the individual trees. This model is known for its robustness and ability to handle complex relationships in the data.

#### Model Building and Evaluation

We split the data into training and testing sets, fit the Random Forest model and then evaluated the model using performance metrics such as R-squared, RMSE, and MAE to assess its predictive accuracy and explanatory power.
```{r eval=FALSE}
#split the data in training and test set 0.8/0.2
set.seed(123)  # for reproducibility
trainIndex <- createDataPartition(properties_filtered$price, p=0.8, list=FALSE)
trainData <- properties_filtered[trainIndex, ]
testData <- properties_filtered[-trainIndex, ]

#apply the RF model as a regression
rf_model <- randomForest(price ~., data=trainData, ntree=500, importance=TRUE)
rf_model.pred_rf <- predict(rf_model, newdata=testData)


rmse_rf <- sqrt(mean((testData$price - rf_model.pred_rf)^2))
mae_rf <- mean(abs(testData$price - rf_model.pred_rf))
r_sq_rf <- cor(testData$price, rf_model.pred_rf)^2

# show those metrics in a html table
metrics_rf <- kable(data.frame(r_sq_rf, rmse_rf, mae_rf), format = "html", col.names = c("R-Squared", "RMSE", "MAE")) %>%
  kable_styling(position = "center", bootstrap_options = c("striped", "bordered", "hover", "condensed")) %>%
  add_header_above(c("Random Forest Model Performance Metrics" = 3))
metrics_rf
```
Comparing with the best model of the linear regression, we can see that the Random Forest model has a higher R-squared value and lower RMSE and MAE values, indicating better predictive accuracy.
```{r eval=FALSE}
metrics_2
```

The plot shows the actual vs. predicted prices, with the diagonal line indicating perfect predictions.
```{r eval=FALSE}
plot(testData$price ~rf_model.pred_rf, col = 'skyblue',
     xlab = 'Actual Price', ylab = 'Predicted Price',
     main = 'Actual vs Predicted Price')

abline(0,1, col = 'darkred')
```


#### Variable Importance
VI plots are a useful tool to understand the relative importance of predictors in the Random Forest model. This plot shows the importance of each predictor in the model, helping to identify key features that drive price predictions.
```{r eval=FALSE}
varImpPlot(rf_model)
importance(rf_model)
```
We see that square_meters is the most important predictor.

#### Cross-Validation

```{r eval=FALSE}
# cv_results_rf <- train(price ~., data=trainData, method="rf", trControl=trainControl(method="cv", number=5))
# summary(cv_results_rf)
# 
# #show the CV result in the html
# metrics_cv_rf <- kable(cv_results_rf$results, format = "html") %>%
#   kable_styling(position = "center", bootstrap_options = c("striped", "bordered", "hover", "condensed"))  %>%
#   add_header_above(c("10 Fold Cross Validation Metrics (Random Forest)" = 7))
# metrics_cv_rf
```


#### Hyperparameter Tuning
```{r eval=FALSE}
# # Define the tuning grid
# tuneGrid <- expand.grid(mtry = seq(2, sqrt(ncol(trainData)), by = 1))  # Tune over a range of mtry values
# 
# # Train the model with tuning
# rf_tuned <- train(price ~ ., data = trainData, method = "rf", 
#                   trControl = trainControl(method = "cv", number = 5, search = "grid"), 
#                   tuneGrid = tuneGrid, 
#                   ntree = 1000)
# 
# # Plotting the tuning effect
# plot(rf_tuned)
# 
# # Evaluate the tuned model
# rf_model_pred <- predict(rf_tuned, newdata = testData)
# rmse_rf <- sqrt(mean((testData$price - rf_model_pred)^2))
# mae_rf <- mean(abs(testData$price - rf_model_pred))
# r_sq_rf <- cor(testData$price, rf_model_pred)^2
# 
# # Show metrics
# metrics_rf <- kable(data.frame(R_Squared = r_sq_rf, RMSE = rmse_rf, MAE = mae_rf),
#                     format = "html", col.names = c("R-Squared", "RMSE", "MAE")) %>%
#              kable_styling(position = "center", bootstrap_options = c("striped", "bordered", "hover", "condensed")) %>%
#              add_header_above(c("Tuned Random Forest Model Performance Metrics" = 3))
# metrics_rf
```

